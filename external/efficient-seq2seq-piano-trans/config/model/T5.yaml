# model:
  model_name: Transformer-T5
  vocab_size: 1024
  # """
  # Token Types:
  # 1) Instrument(128 values)
  # 2) Note(128 values)
  # 3) On/Off(2 values)
  # 4) Time(205 values)
  # 5) Drum(128 values)
  # 6) End Tie Section(1 value)
  # 7) EOS(1 value)
  # """
  encoder_name: "TransformerEncoder" # "TransformerEncoder" # "CNNFreqTransEncoder" # "CNNEncoder", "TransformerEncoder"
  decoder_name: "TransformerDecoder" # "TransformerDecoder", "FrameLvTransDec"
  encoder_attention: "Multi_Head_Attention" # Multi_Head_Attention, RelativeGlobalAttention
  encoder_position_emb: true
  encoder_input_dim: 512
  emb_dim: 512
  num_heads: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  head_dim: 64
  mlp_dim: 1024
  mlp_activations: relu
  dropout_rate: 0.1
  logits_via_embeddings: False
  dtype: torch.float32
  # dtype: torch.float16  


#HRM 
  use_hrm_refiner: true
  hrm_H_cycles: 2
  hrm_L_cycles: 2
  hrm_H_layers: 4
  hrm_L_layers: 4
  hrm_num_heads: 8
  hrm_expansion: 4.0
  hrm_rms_norm_eps: 1e-5
  hrm_use_rope: true
  hrm_forward_dtype: torch.bfloat16

  hrm_halt_max_steps: 1
  hrm_halt_exploration_prob: 0.0
  hrm_use_act_halt: false
  hrm_cross_scale_coupling: false
  hrm_cross_scale_bidirectional: true
  hrm_ablation_pooling_sets: []
