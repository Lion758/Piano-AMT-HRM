defaults:
  - experiment_T5_V3_HybridGlobalLocalCrossAttn

model:
  checkpoint_path:  "checkpoints/T5_V4_steps_200000.ckpt"

  cross_attention_hierarchy_pooling: true
  pooling_sizes: [4, 4, 2, 2, 1, 1]
  hrm_refine_scales: [4, 2, 1]
  hrm_cross_scale_coupling: true
  hrm_cross_scale_bidirectional: true
  hrm_ablation_pooling_sets:
    - [4, 2, 1]
    - [4, 2]
    - [2, 1]
  # pooling_sizes: [4, 4, 4, 4, 4, 4]

training:
  notes: "HRM_adapter_with_freezing_USE_ROPE=true"
  

